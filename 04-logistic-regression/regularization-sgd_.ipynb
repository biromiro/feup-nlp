{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notebook prepared by Henrique Lopes Cardoso (hlc@fe.up.pt).\n",
    "\n",
    "# REGULARIZATION AND SGD\n",
    "\n",
    "Regularization is a technique that allows us to avoid overfitting by penalizing excessive feature weights. Several classifiers, such as [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html),  include the option for choosing which regularization term to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll explore the usage of different regularization terms. For that, we'll use a restaurant reviews classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    500\n",
      "0    500\n",
      "Name: Liked, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>Liked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Wow... Loved this place.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Crust is not good.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Not tasty and the texture was just nasty.</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stopped by during the late May bank holiday of...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The selection on the menu was great and so wer...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              Review  Liked\n",
       "0                           Wow... Loved this place.      1\n",
       "1                                 Crust is not good.      0\n",
       "2          Not tasty and the texture was just nasty.      0\n",
       "3  Stopped by during the late May bank holiday of...      1\n",
       "4  The selection on the menu was great and so wer...      1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the data\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Importing the dataset\n",
    "dataset = pd.read_csv('Restaurant_Reviews.tsv', delimiter = '\\t', quoting = 3)\n",
    "\n",
    "print(dataset['Liked'].value_counts())\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning the text\n",
    "\n",
    "import re\n",
    "import nltk\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "corpus = []\n",
    "ps = PorterStemmer()\n",
    "for i in range(0,1000):\n",
    "    # get review and remove non alpha chars\n",
    "    review = re.sub('[^a-zA-Z]', ' ', dataset['Review'][i])\n",
    "    # to lower-case and tokenize\n",
    "    review = review.lower().split()\n",
    "    # stemming and stop word removal\n",
    "    review = ' '.join([ps.stem(w) for w in review if not w in set(stopwords.words('english'))])\n",
    "    corpus.append(review)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 1500) (1000,)\n"
     ]
    }
   ],
   "source": [
    "# Creating a bag-of-words model\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(max_features = 1500)\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = dataset['Liked']\n",
    "\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 1500) (800,)\n",
      "(200, 1500) (200,)\n",
      "1    400\n",
      "0    400\n",
      "Name: Liked, dtype: int64\n",
      "0    100\n",
      "1    100\n",
      "Name: Liked, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Splitting the dataset into training and test sets\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state = 0, stratify=y)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_test.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression\n",
    "\n",
    "Scikit-learn's [Logistic Regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) includes both L1 and L2 regularizations. L2 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score\n",
    "\n",
    "clf = LogisticRegression(penalty='l2') # l2 regularization is the default\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression with L2 regularization\n",
      "Accuracy: 0.805\n",
      "Precision: 0.8210526315789474\n",
      "Recall: 0.78\n",
      "F1: 0.8\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"LogisticRegression with L2 regularization\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the feature weights that we've obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.41686797,  0.17697687,  0.        , ..., -0.20321363,\n",
       "        0.64383437, -0.61415454])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "fw = clf.coef_[0]\n",
    "fw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are actually being used? (I.e., how many non-zero weights are there?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1311"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "len([w for w in fw if w != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L1 regularization typically obtains sparser weight vectors. Try using L1 regularization (check the documentation for additional changes you might need). How many non-zero weights do you have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "149"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "clf2 = LogisticRegression(penalty='l1', solver='liblinear')\n",
    "clf2.fit(X_train, y_train)\n",
    "\n",
    "fw2 = clf2.coef_[0]\n",
    "len([w for w in fw2 if w != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression with L1 regularization\n",
      "Accuracy: 0.795\n",
      "Precision: 0.8641975308641975\n",
      "Recall: 0.7\n",
      "F1: 0.7734806629834253\n"
     ]
    }
   ],
   "source": [
    "y_pred2 = clf2.predict(X_test)\n",
    "\n",
    "print(\"LogisticRegression with L1 regularization\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred2)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred2)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred2)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred2)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also try using a mix of L1 and L2 (check the documentation for how to do it)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "380"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "clf3 = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=10000)\n",
    "clf3.fit(X_train, y_train)\n",
    "y_pred = clf3.predict(X_test)\n",
    "\n",
    "fw3 = clf3.coef_[0]\n",
    "len([w for w in fw3 if w != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegression with L1 and L2 regularization with an l1_ratio of 0.5\n",
      "Accuracy: 0.78\n",
      "Precision: 0.8181818181818182\n",
      "Recall: 0.72\n",
      "F1: 0.7659574468085107\n"
     ]
    }
   ],
   "source": [
    "y_pred3 = clf3.predict(X_test)\n",
    "\n",
    "print(\"LogisticRegression with L1 and L2 regularization with an l1_ratio of 0.5\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred3)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred3)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred3)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred3)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVM\n",
    "\n",
    "Scikit-learn's [SVM](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html) also includes both L1 and L2 regularizations. L2 is the default."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[82 18]\n",
      " [20 80]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "clf = LinearSVC(penalty='l2') # l2 regularization is the default\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with L2 regularization\n",
      "Accuracy: 0.81\n",
      "Precision: 0.8163265306122449\n",
      "Recall: 0.8\n",
      "F1: 0.8080808080808082\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"SVM with L2 regularization\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many features are actually being used? (I.e., how many non-zero weights are there?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1083"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "len([w for w in clf.coef_[0] if w != 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using L1 regularization (check the documentation for additional changes you might need). How many non-zero weights do you have now?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86 14]\n",
      " [29 71]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "418"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf = LinearSVC(penalty='l1', loss='squared_hinge', dual=False) # l2 regularization is the default\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "len([w for w in clf.coef_[0] if w != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM with L1 regularization\n",
      "Accuracy: 0.785\n",
      "Precision: 0.8352941176470589\n",
      "Recall: 0.71\n",
      "F1: 0.7675675675675675\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"SVM with L1 regularization\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD Classifier\n",
    "\n",
    "Scikit-learn's [SGD Classifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html) implements regularized linear models (such as SVM and Logistic Regression) with stochastic gradient descent (SGD) learning: the gradient of the loss is estimated each sample at a time and the model is updated along the way with a decreasing learning rate.\n",
    "\n",
    "Several loss functions can be used, namely *hinge loss* (which corresponds to SVM) and *log loss* (which corresponds to Logistic Regression). And as before, you can use L1 and/or L2 regularization.\n",
    "\n",
    "The *max_iter* parameter allows you to set the maximum number of epochs, where an epoch corresponds to going through the whole dataset for training. Also, *learning_rate* allows you to set a learning rate schedule.\n",
    "\n",
    "Several parameters allow you to define stopping criteria: *tol* specifies a tolerance loss value or stopping criterion, while *n_iter_no_change* indicates the number of iterations with no improvement that should be observed before stopping; *early_stopping* allows us to use a validation set (a fraction *validation_fraction* of the training data) on which the stopping criterion will be checked (instead of checking the loss on the training data).\n",
    "\n",
    "The *verbose* parameter allows you to set a verbosity (output) level.\n",
    "\n",
    "Try using SGD, and explore different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Epoch 1\n",
      "Norm: 168.60, NNZs: 690, Bias: -5.551294, T: 800, Avg. loss: 3.824504\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 2\n",
      "Norm: 147.52, NNZs: 886, Bias: -0.005331, T: 1600, Avg. loss: 1.362331\n",
      "Total training time: 0.00 seconds.\n",
      "-- Epoch 3\n",
      "Norm: 123.74, NNZs: 951, Bias: -0.024917, T: 2400, Avg. loss: 0.507045\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 4\n",
      "Norm: 106.00, NNZs: 978, Bias: 2.325033, T: 3200, Avg. loss: 0.262562\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 5\n",
      "Norm: 92.60, NNZs: 991, Bias: -0.171952, T: 4000, Avg. loss: 0.156892\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 6\n",
      "Norm: 82.38, NNZs: 1008, Bias: -0.256581, T: 4800, Avg. loss: 0.105431\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 7\n",
      "Norm: 74.95, NNZs: 1022, Bias: -1.821244, T: 5600, Avg. loss: 0.060162\n",
      "Total training time: 0.01 seconds.\n",
      "-- Epoch 8\n",
      "Norm: 68.41, NNZs: 1027, Bias: -0.339899, T: 6400, Avg. loss: 0.049913\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 9\n",
      "Norm: 62.88, NNZs: 1031, Bias: -0.319476, T: 7200, Avg. loss: 0.028808\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 10\n",
      "Norm: 58.17, NNZs: 1032, Bias: -0.316558, T: 8000, Avg. loss: 0.020292\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 11\n",
      "Norm: 53.98, NNZs: 1032, Bias: -0.344052, T: 8800, Avg. loss: 0.014433\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 12\n",
      "Norm: 50.76, NNZs: 1035, Bias: -1.319136, T: 9600, Avg. loss: 0.021601\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 13\n",
      "Norm: 47.57, NNZs: 1036, Bias: -1.283511, T: 10400, Avg. loss: 0.021607\n",
      "Total training time: 0.02 seconds.\n",
      "-- Epoch 14\n",
      "Norm: 44.82, NNZs: 1036, Bias: 0.400472, T: 11200, Avg. loss: 0.013487\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 15\n",
      "Norm: 42.16, NNZs: 1036, Bias: -1.185191, T: 12000, Avg. loss: 0.009858\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 16\n",
      "Norm: 39.91, NNZs: 1036, Bias: -0.413638, T: 12800, Avg. loss: 0.010840\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 17\n",
      "Norm: 37.87, NNZs: 1036, Bias: -0.416664, T: 13600, Avg. loss: 0.009671\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 18\n",
      "Norm: 36.11, NNZs: 1036, Bias: -0.421866, T: 14400, Avg. loss: 0.004946\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 19\n",
      "Norm: 34.49, NNZs: 1036, Bias: -1.056011, T: 15200, Avg. loss: 0.011344\n",
      "Total training time: 0.03 seconds.\n",
      "-- Epoch 20\n",
      "Norm: 33.11, NNZs: 1037, Bias: -0.445988, T: 16000, Avg. loss: 0.007322\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 21\n",
      "Norm: 31.68, NNZs: 1037, Bias: -1.014360, T: 16800, Avg. loss: 0.004497\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 22\n",
      "Norm: 30.53, NNZs: 1037, Bias: -0.453892, T: 17600, Avg. loss: 0.008093\n",
      "Total training time: 0.04 seconds.\n",
      "-- Epoch 23\n",
      "Norm: 29.37, NNZs: 1037, Bias: -0.461044, T: 18400, Avg. loss: 0.007292\n",
      "Total training time: 0.04 seconds.\n",
      "Convergence after 23 epochs took 0.04 seconds\n",
      "[[78 22]\n",
      " [18 82]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1037"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# your code here\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "clf = SGDClassifier(loss='perceptron', verbose=1)\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "len([w for w in clf.coef_[0] if w != 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD with L2 regularization and tolereance of 1e-3\n",
      "Accuracy: 0.8\n",
      "Precision: 0.7884615384615384\n",
      "Recall: 0.82\n",
      "F1: 0.803921568627451\n"
     ]
    }
   ],
   "source": [
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "print(\"SGD with L2 regularization and tolereance of 1e-3\")\n",
    "\n",
    "# Assess the accuracy, precision, recall, and F1 score of the model\n",
    "print(f'Accuracy: {accuracy_score(y_test, y_pred)}')\n",
    "print(f'Precision: {precision_score(y_test, y_pred)}')\n",
    "print(f'Recall: {recall_score(y_test, y_pred)}')\n",
    "print(f'F1: {f1_score(y_test, y_pred)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stochastic gradient descent updates the model weights base on one example at a time. Instead, we can compute the gradient over batches of training instances before updating the weights.\n",
    "\n",
    "SGDClassifier allows us to do so via [*partial_fit*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html#sklearn.linear_model.SGDClassifier.partial_fit), which corresponds to training the model with a specific set of examples for a single epoch. To properly use this method, we need to split our data into mini-batches and then iterate through them for as many epochs as we want.\n",
    "Matters such as objective convergence, early stopping, and learning rate adjustments must be handled manually.\n",
    "\n",
    "Try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.53\n",
      "0.54\n",
      "0.635\n",
      "0.57\n",
      "0.63\n",
      "0.64\n",
      "0.565\n",
      "0.645\n",
      "0.635\n",
      "0.605\n",
      "0.645\n",
      "0.675\n",
      "0.655\n",
      "0.585\n",
      "0.7\n",
      "0.625\n",
      "0.61\n",
      "0.605\n",
      "0.63\n",
      "0.675\n",
      "0.615\n",
      "0.705\n",
      "0.685\n",
      "0.705\n",
      "0.705\n",
      "0.675\n",
      "0.68\n",
      "0.715\n",
      "0.71\n",
      "0.71\n",
      "0.69\n",
      "0.695\n",
      "0.735\n",
      "0.7\n",
      "0.69\n",
      "0.68\n",
      "0.715\n",
      "0.68\n",
      "0.65\n",
      "0.715\n",
      "0.665\n",
      "0.67\n",
      "0.625\n",
      "0.675\n",
      "0.715\n",
      "0.64\n",
      "0.69\n",
      "0.665\n",
      "0.655\n",
      "0.685\n",
      "0.685\n",
      "0.695\n",
      "0.715\n",
      "0.7\n",
      "0.73\n",
      "0.725\n",
      "0.725\n",
      "0.705\n",
      "0.72\n",
      "0.735\n",
      "0.755\n",
      "0.725\n",
      "0.705\n",
      "0.7\n",
      "0.71\n",
      "0.745\n",
      "0.76\n",
      "0.75\n",
      "0.745\n",
      "0.71\n",
      "0.725\n",
      "0.735\n",
      "0.72\n",
      "0.745\n",
      "0.695\n",
      "0.71\n",
      "0.755\n",
      "0.735\n",
      "0.735\n",
      "0.725\n",
      "0.67\n",
      "0.72\n",
      "0.705\n",
      "0.72\n",
      "0.69\n",
      "0.705\n",
      "0.705\n",
      "0.665\n",
      "0.72\n",
      "0.69\n",
      "0.72\n",
      "0.73\n",
      "0.72\n",
      "0.73\n",
      "0.725\n",
      "0.755\n",
      "0.755\n",
      "0.735\n",
      "0.745\n",
      "0.75\n",
      "0.68\n",
      "0.68\n",
      "0.725\n",
      "0.705\n",
      "0.71\n",
      "0.75\n",
      "0.75\n",
      "0.74\n",
      "0.745\n",
      "0.725\n",
      "0.75\n",
      "0.735\n",
      "0.73\n",
      "0.75\n",
      "0.745\n",
      "0.74\n",
      "0.755\n",
      "0.73\n",
      "0.695\n",
      "0.72\n",
      "0.73\n",
      "0.755\n",
      "0.745\n",
      "0.775\n",
      "0.765\n",
      "0.72\n",
      "0.765\n",
      "0.745\n",
      "0.765\n",
      "0.745\n",
      "0.69\n",
      "0.705\n",
      "0.745\n",
      "0.78\n",
      "0.775\n",
      "0.78\n",
      "0.78\n",
      "0.69\n",
      "0.755\n",
      "0.74\n",
      "0.75\n",
      "0.77\n",
      "0.76\n",
      "0.73\n",
      "0.73\n",
      "0.73\n",
      "0.74\n",
      "0.745\n",
      "0.71\n",
      "0.755\n",
      "0.71\n",
      "0.715\n",
      "0.745\n",
      "0.76\n",
      "0.775\n",
      "0.695\n",
      "0.765\n",
      "0.765\n",
      "0.755\n",
      "0.745\n",
      "0.74\n",
      "0.755\n",
      "0.74\n",
      "0.74\n",
      "0.725\n",
      "0.72\n",
      "0.72\n",
      "0.74\n",
      "0.735\n",
      "0.71\n",
      "0.77\n",
      "0.77\n",
      "0.775\n",
      "0.775\n",
      "0.77\n",
      "0.765\n",
      "0.765\n",
      "0.775\n",
      "0.765\n",
      "0.75\n",
      "0.705\n",
      "0.705\n",
      "0.705\n",
      "0.695\n",
      "0.735\n",
      "0.76\n",
      "0.735\n",
      "0.73\n",
      "0.76\n",
      "0.74\n",
      "0.745\n",
      "0.71\n",
      "0.745\n",
      "0.755\n",
      "0.745\n",
      "0.78\n",
      "0.775\n",
      "0.77\n",
      "0.73\n",
      "0.78\n",
      "0.77\n",
      "0.76\n",
      "0.755\n",
      "0.78\n",
      "0.79\n",
      "0.785\n",
      "0.785\n",
      "0.78\n",
      "0.745\n",
      "0.745\n",
      "0.715\n",
      "0.73\n",
      "0.74\n",
      "0.76\n",
      "0.77\n",
      "0.78\n",
      "0.75\n",
      "0.76\n",
      "0.76\n",
      "0.76\n",
      "0.76\n",
      "0.755\n",
      "0.77\n",
      "0.765\n",
      "0.765\n",
      "0.695\n",
      "0.73\n",
      "0.725\n",
      "0.765\n",
      "0.8\n",
      "0.795\n",
      "0.785\n",
      "0.785\n",
      "0.79\n",
      "0.785\n",
      "0.785\n",
      "0.79\n",
      "0.795\n",
      "0.775\n",
      "0.79\n",
      "0.775\n",
      "0.81\n",
      "0.805\n",
      "0.8\n",
      "0.795\n",
      "0.76\n",
      "0.81\n",
      "0.78\n",
      "0.805\n",
      "0.78\n",
      "0.79\n",
      "0.81\n",
      "0.81\n",
      "0.81\n",
      "0.805\n",
      "0.745\n",
      "0.745\n",
      "0.805\n",
      "0.805\n",
      "0.76\n",
      "0.795\n",
      "0.79\n",
      "0.735\n",
      "0.71\n",
      "0.81\n",
      "0.82\n",
      "0.81\n",
      "0.81\n",
      "0.79\n",
      "0.78\n",
      "0.815\n",
      "0.81\n",
      "0.795\n",
      "0.805\n",
      "0.81\n",
      "0.815\n",
      "0.79\n",
      "0.795\n",
      "0.795\n",
      "0.8\n",
      "0.8\n",
      "0.785\n",
      "0.805\n",
      "0.795\n",
      "0.8\n",
      "0.765\n",
      "0.8\n",
      "0.795\n",
      "0.78\n",
      "0.76\n",
      "0.725\n",
      "0.765\n",
      "0.775\n",
      "0.8\n",
      "0.8\n",
      "0.78\n",
      "0.78\n",
      "0.795\n",
      "0.795\n",
      "0.8\n",
      "0.815\n",
      "0.81\n",
      "0.81\n",
      "0.795\n",
      "0.79\n",
      "0.755\n",
      "0.755\n",
      "0.76\n",
      "0.77\n",
      "0.79\n",
      "0.745\n",
      "0.75\n",
      "0.795\n",
      "0.79\n",
      "0.79\n",
      "0.775\n",
      "0.795\n",
      "0.79\n",
      "0.8\n",
      "0.815\n",
      "0.805\n",
      "0.81\n",
      "0.825\n",
      "0.79\n",
      "0.79\n",
      "0.79\n",
      "0.815\n",
      "0.785\n",
      "0.82\n",
      "0.795\n",
      "0.79\n",
      "0.79\n",
      "0.79\n",
      "0.795\n",
      "0.795\n",
      "0.775\n",
      "0.775\n",
      "0.775\n",
      "0.81\n",
      "0.8\n",
      "0.765\n",
      "0.77\n",
      "0.74\n",
      "0.705\n",
      "0.825\n",
      "0.805\n",
      "0.83\n",
      "0.82\n",
      "0.83\n",
      "0.81\n",
      "0.795\n",
      "0.8\n",
      "0.83\n",
      "0.825\n",
      "0.825\n",
      "0.79\n",
      "0.79\n",
      "0.795\n",
      "0.805\n",
      "0.815\n",
      "0.82\n",
      "0.82\n",
      "0.81\n",
      "0.82\n",
      "0.82\n",
      "0.78\n",
      "0.78\n",
      "0.78\n",
      "0.815\n",
      "0.815\n",
      "0.735\n",
      "0.735\n",
      "0.78\n",
      "0.795\n",
      "0.795\n",
      "0.805\n",
      "0.805\n",
      "0.8\n",
      "0.8\n",
      "0.81\n",
      "0.795\n",
      "0.795\n",
      "0.805\n",
      "0.78\n",
      "0.775\n",
      "0.76\n",
      "0.76\n",
      "0.77\n",
      "0.79\n",
      "0.8\n",
      "0.785\n",
      "0.795\n",
      "0.805\n",
      "0.82\n",
      "0.81\n",
      "0.79\n",
      "0.8\n",
      "0.805\n",
      "0.825\n",
      "0.815\n",
      "0.82\n",
      "0.82\n",
      "0.815\n",
      "0.825\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.81\n",
      "0.815\n",
      "0.79\n",
      "0.79\n",
      "0.79\n",
      "0.79\n",
      "0.81\n",
      "0.8\n",
      "0.735\n",
      "0.755\n",
      "0.755\n",
      "0.78\n",
      "0.78\n",
      "0.765\n",
      "0.75\n",
      "0.755\n",
      "0.74\n",
      "0.79\n",
      "0.82\n",
      "0.82\n",
      "0.825\n",
      "0.825\n",
      "0.825\n",
      "0.815\n",
      "0.81\n",
      "0.83\n",
      "0.825\n",
      "0.815\n",
      "0.82\n",
      "0.81\n",
      "0.815\n",
      "0.825\n",
      "0.825\n",
      "0.825\n",
      "0.82\n",
      "0.825\n",
      "0.82\n",
      "0.825\n",
      "0.79\n",
      "0.81\n",
      "0.805\n",
      "0.83\n",
      "0.82\n",
      "0.745\n",
      "0.75\n",
      "0.775\n",
      "0.79\n",
      "0.79\n",
      "0.81\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.82\n",
      "0.805\n",
      "0.81\n",
      "0.8\n",
      "0.805\n",
      "0.805\n",
      "0.78\n",
      "0.785\n",
      "0.785\n",
      "0.785\n",
      "0.8\n",
      "0.79\n",
      "0.79\n",
      "0.79\n",
      "0.795\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.8\n",
      "0.825\n",
      "0.82\n"
     ]
    }
   ],
   "source": [
    "# your code here\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "def batch(iterable_X, iterable_y, n=1):\n",
    "    l = len(iterable_X)\n",
    "    for ndx in range(0, l, n):\n",
    "        yield iterable_X[ndx:min(ndx + n, l)], iterable_y[ndx:min(ndx + n, l)]\n",
    "\n",
    "clf = SGDClassifier(alpha=.0001, loss='log_loss', penalty='l2', n_jobs=-1, shuffle=True, max_iter=100, verbose=0, tol=0.001)\n",
    " \n",
    "ROUNDS = 20\n",
    "for _ in range(ROUNDS):\n",
    "    batcherator = batch(X_train, y_train, 10)\n",
    "    for index, (chunk_X, chunk_y) in enumerate(batcherator):\n",
    "        clf.partial_fit(chunk_X, chunk_y, classes=[0, 1])\n",
    " \n",
    "        y_predicted = clf.predict(X_test)\n",
    "        print(accuracy_score(y_test, y_predicted))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "977cd4336f4538d170b2e975b8afcec38d10dc2076c29b717fcf78d7fa79bfff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
